#+TITLE: Notes
* On Lazy observations
As-is, lazy observations are a requirement in order to statically reason
(really, "assume") a trace's structure before running it. The core of why this
is happening is really surrounding nesting of program.

Maybe combinators doesn't need them and they are only sugar for defining
generative models (in which case tests need to be double-checked for later).

** On the planned removal
the main design question is "does the creation of an enqueue observe statement
produce a new model?" two options here:
- no: you will need to wrap your generative models in new another program to
  keep observations persistent.
- yes: requires more infrastructure

* Proto work
#+begin_example python
@combinator(data='x')
def p(g, x):
    z = sample(Normal(0, 1))
    a = sample(Normal(z, 1))
    observe(Normal(g(z), 1), x)
    return x, z


def combinator(fn, data=None):
    initializer = partial(P.__init__, runnable_thing=fn)
    return initializer # .__call__

def q(e, f, x):
    m, s = e(x)
    b = sample(Normal(m, s))
    u = sample(Normal(b, 1))
    factor(f(u, x))
    return u, x

def f(k, u, x):
    m, s = k(u, x)
    z = sample(Normal(m, s))
    return z

def r(k, x, z):
    m, s = k(z, x)
    u = sample(Normal(m, s))
    return u

propose(reverse(p, r), forward(q, f))(g, e, k, x)  # -> 'data'
#+end_example


OO-style:
#+begin_example python

class P(nn.ProbTorchModule):
    def __init__(self):
        self.g = ...
    def forward(self, x):
        z = self.sample("z", Normal(0, 1))
        a = self.sample("a", Normal(z, 1))
        self.observe("x", Normal(g(z), 1), x)
        return x, z

class Q:
    ...

class F:
    ...

class K:
    ...

#+end_example

#+begin_example
propose(reverse(P(), R()), forward(Q(), F()))(x, 4, dict(s=10))  # -> 'data'
                |                  |     \
                |           _______:_____ `,
                |           u, b = Q()(x)  |
                |                    ______:______
            ____:________            z = F()(u, x)
            x, z = P()(x)

forward(x):
  u, b = Q()(x)
  z    = F()(u, x)
  return ?

reverse(x):
  x', z = P()(x)
  u     = R()(x', z)
  return x'

propose:
  reverse(x)
#+end_example

# 1d gaussian
#  - pi_1 1d gaus mean 0
#  - pi_2 1d gaus mean 1   <<< at one step no need for detaches in the NVI step (only if you don't compute normalizing constants)
#  - pi_3 1d gaus mean 2
#  - pi_4 1d gaus mean 3
#
# NVI stuff -- target and proposal always fixed
#           -- detaches happen in between (don't forget)
#
# 1-step NVI (VAE)
# 3-step NVI (NVI-sequential): 4 intermediate densities
