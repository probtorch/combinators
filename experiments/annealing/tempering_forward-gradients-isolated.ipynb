{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'targets': [Normal(name=g0, loc=0.0, scale=1.0),\n",
       "  [Î²=0.5000][g1]Tempered(\n",
       "    (density1): Normal(name=g0, loc=0.0, scale=1.0)\n",
       "    (density2): Normal(name=g2, loc=6.0, scale=1.0)\n",
       "  ),\n",
       "  Normal(name=g2, loc=6.0, scale=1.0)],\n",
       " 'forwards': [ext_to=g1:NormalLinearKernel(\n",
       "    (net): LinearMap(\n",
       "      (net): Linear(in_features=1, out_features=1, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  ext_to=g2:NormalLinearKernel(\n",
       "    (net): LinearMap(\n",
       "      (net): Linear(in_features=1, out_features=1, bias=True)\n",
       "    )\n",
       "  )],\n",
       " 'reverses': [ext_to=g0:NormalLinearKernel(\n",
       "    (net): LinearMap(\n",
       "      (net): Linear(in_features=1, out_features=1, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  ext_to=g1:NormalLinearKernel(\n",
       "    (net): LinearMap(\n",
       "      (net): Linear(in_features=1, out_features=1, bias=True)\n",
       "    )\n",
       "  )]}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import torch\n",
    "import math\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import trange\n",
    "from typing import Tuple\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import combinators.trace.utils as trace_utils\n",
    "from combinators.trace.utils import RequiresGrad\n",
    "from combinators.tensor.utils import autodevice, kw_autodevice, copy, show\n",
    "from combinators.densities import MultivariateNormal, Tempered, RingGMM, Normal\n",
    "from combinators.densities.kernels import MultivariateNormalKernel, MultivariateNormalLinearKernel, NormalLinearKernel\n",
    "from combinators.nnets import ResMLPJ\n",
    "from combinators.objectives import nvo_rkl, nvo_avo\n",
    "from combinators import Forward, Reverse, Propose\n",
    "from combinators.stochastic import RandomVariable, ImproperRandomVariable\n",
    "from combinators.metrics import effective_sample_size, log_Z_hat\n",
    "import visualize as V\n",
    "\n",
    "def mk_kernel(from_:int, to_:int, std:float, num_hidden:int, learn_cov=True):\n",
    "    embedding_dim = 2\n",
    "    return MultivariateNormalKernel(\n",
    "        ext_from=f'g{from_}',\n",
    "        ext_to=f'g{to_}',\n",
    "        loc=torch.zeros(2, **kw_autodevice()),\n",
    "        cov=torch.eye(2, **kw_autodevice())*std**2,\n",
    "        learn_cov=learn_cov,\n",
    "        net=ResMLPJ(\n",
    "            dim_in=2,\n",
    "            dim_hidden=num_hidden,\n",
    "            dim_out=embedding_dim).to(autodevice()))\n",
    "\n",
    "def mk_mnlinear_kernel(from_:int, to_:int, std:float, dim:int):\n",
    "    return MultivariateNormalLinearKernel(\n",
    "        ext_from=f'g{from_}',\n",
    "        ext_to=f'g{to_}',\n",
    "        loc=torch.zeros(dim, **kw_autodevice()),\n",
    "        cov=torch.eye(dim, **kw_autodevice())*std**2)\n",
    "\n",
    "def mk_nlinear_kernel(from_:int, to_:int, std:float, dim:int):\n",
    "    return NormalLinearKernel(ext_from=f'g{from_}', ext_to=f'g{to_}')\n",
    "\n",
    "def mk_targets(num_targets):\n",
    "    proposal_std = 8\n",
    "    g0 = MultivariateNormal(name='g0', loc=torch.zeros(2, **kw_autodevice()), cov=torch.eye(2, **kw_autodevice())*4**2)\n",
    "    gK = MultivariateNormal(name=f\"g{num_targets - 1}\", loc=torch.ones(2, **kw_autodevice())*4, cov=torch.eye(2, **kw_autodevice())**2)\n",
    "\n",
    "    # Make an annealing path\n",
    "    betas = torch.arange(0., 1., 1./(num_targets - 1))[1:] # g_0 is beta=0\n",
    "    path = [Tempered(f'g{k}', g0, gK, beta) for k, beta in zip(range(1,num_targets-1), betas)]\n",
    "    path = [g0] + path + [gK]\n",
    "    assert len(path) == num_targets # sanity check that the betas line up\n",
    "    return path\n",
    "\n",
    "def anneal_between(left, right, total_num_targets):\n",
    "    proposal_std = total_num_targets\n",
    "\n",
    "    # Make an annealing path\n",
    "    betas = torch.arange(0., 1., 1./(total_num_targets - 1))[1:] # g_0 is beta=0\n",
    "    path = [Tempered(f'g{k}', left, right, beta) for k, beta in zip(range(1,total_num_targets-1), betas)]\n",
    "    path = [left] + path + [right]\n",
    "    \n",
    "    assert len(path) == total_num_targets # sanity check that the betas line up\n",
    "    return path\n",
    "\n",
    "\n",
    "def anneal_between_mvns(left_loc, right_loc, total_num_targets):\n",
    "    g0 = mk_mvn(0, left_loc)\n",
    "    gK =  mk_mvn(total_num_targets-1, right_loc)\n",
    "\n",
    "    return anneal_between(g0, gK, total_num_targets)\n",
    "\n",
    "def anneal_between_ns(left_loc, right_loc, total_num_targets):\n",
    "    g0 = mk_n(0, left_loc)\n",
    "    gK =  mk_n(total_num_targets-1, right_loc)\n",
    "\n",
    "    return anneal_between(g0, gK, total_num_targets)\n",
    "\n",
    "def mk_mvn(i, loc):\n",
    "    return MultivariateNormal(name=f'g{i}', loc=torch.ones(2, **kw_autodevice())*loc, cov=torch.eye(2, **kw_autodevice())**2)\n",
    "\n",
    "def mk_n(i, loc):\n",
    "    return Normal(name=f'g{i}', loc=torch.ones(1, **kw_autodevice())*loc, scale=torch.ones(1, **kw_autodevice())**2)\n",
    "\n",
    "def mk_model(num_targets:int):\n",
    "    return dict(\n",
    "#         targets=mk_targets(num_targets),\n",
    "#         forwards=[mk_kernel(from_=i, to_=i+1, std=1., num_hidden=64) for i in range(num_targets-1)],\n",
    "#         reverses=[mk_kernel(from_=i+1, to_=i, std=1., num_hidden=64) for i in range(num_targets-1)],\n",
    "\n",
    "#         targets=anneal_between_mvns(0, num_targets*2, num_targets),\n",
    "#         forwards=[mk_kernel(from_=i, to_=i+1, std=1., num_hidden=64) for i in range(num_targets-1)],\n",
    "#         reverses=[mk_kernel(from_=i+1, to_=i, std=1., num_hidden=64) for i in range(num_targets-1)],\n",
    "\n",
    "#         targets=anneal_between_mvns(0, num_targets*2, num_targets),\n",
    "#         forwards=[mk_mnlinear_kernel(from_=i, to_=i+1, std=1., dim=2) for i in range(num_targets-1)],\n",
    "#         reverses=[mk_mnlinear_kernel(from_=i+1, to_=i, std=1., dim=2) for i in range(num_targets-1)],\n",
    "\n",
    "        targets=anneal_between_ns(0, num_targets*2, num_targets),\n",
    "        forwards=[mk_nlinear_kernel(from_=i, to_=i+1, std=1., dim=1) for i in range(num_targets-1)],\n",
    "        reverses=[mk_nlinear_kernel(from_=i+1, to_=i, std=1., dim=1) for i in range(num_targets-1)],\n",
    "\n",
    "#         targets=[mk_mvn(i, i*2) for i in range(num_targets)],\n",
    "#         forwards=[mk_kernel(from_=i, to_=i+1, std=1., num_hidden=32) for i in range(num_targets-1)],\n",
    "#         reverses=[mk_kernel(from_=i+1, to_=i, std=1., num_hidden=32) for i in range(num_targets-1)],\n",
    "\n",
    "#         targets=[mk_mvn(i, i*2) for i in range(num_targets)],\n",
    "#         forwards=[mk_mnlinear_kernel(from_=i, to_=i+1, std=1., dim=2) for i in range(num_targets-1)],\n",
    "#         reverses=[mk_mnlinear_kernel(from_=i+1, to_=i, std=1., dim=2) for i in range(num_targets-1)],\n",
    "\n",
    "#         targets=[mk_n(i, i*2) for i in range(num_targets)],\n",
    "#         forwards=[mk_nlinear_kernel(from_=i, to_=i+1, std=1., dim=1) for i in range(num_targets-1)],\n",
    "#         reverses=[mk_nlinear_kernel(from_=i+1, to_=i, std=1., dim=1) for i in range(num_targets-1)],\n",
    "    )\n",
    "K = 3\n",
    "\n",
    "mk_model(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import trange\n",
    "from typing import Tuple\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import combinators.trace.utils as trace_utils\n",
    "from combinators.tensor.utils import autodevice, kw_autodevice\n",
    "from combinators.densities import MultivariateNormal, Tempered, RingGMM\n",
    "from combinators.densities.kernels import MultivariateNormalKernel\n",
    "from combinators.nnets import ResMLPJ\n",
    "from combinators.objectives import nvo_rkl\n",
    "from combinators import Forward, Reverse, Propose\n",
    "from combinators.stochastic import RandomVariable, ImproperRandomVariable\n",
    "from combinators.metrics import effective_sample_size, log_Z_hat\n",
    "import visualize as V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from main import mk_model, mk_kernel\n",
    "from tqdm.notebook import trange, tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from combinators import Forward\n",
    "\n",
    "def sample_along(proposal, kernels, sample_shape=(2000,)):\n",
    "    samples = []\n",
    "    tr, out = proposal(sample_shape=sample_shape)\n",
    "    samples.append(out)\n",
    "    for k in forwards:\n",
    "        proposal = Forward(k, proposal)\n",
    "        tr, out = proposal(sample_shape=sample_shape)\n",
    "        samples.append(out)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main() arguments\n",
    "seed=1\n",
    "eval_break = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "torch.manual_seed(seed)\n",
    "num_samples = 256\n",
    "sample_shape=(num_samples,)\n",
    "\n",
    "# Models\n",
    "out = mk_model(K)\n",
    "targets, forwards, reverses = [[m.to(autodevice()) for m in out[n]] for n in ['targets', 'forwards', 'reverses']]\n",
    "\n",
    "assert all([len(list(k.parameters())) >  0 for k in [*forwards, *reverses]])\n",
    "\n",
    "# logging\n",
    "writer = SummaryWriter()\n",
    "loss_ct, loss_sum, loss_avgs, loss_all = 0, 0.0, [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Normal(name=g0, loc=0.0, scale=1.0), [Î²=0.5000][g1]Tempered(\n",
      "  (density1): Normal(name=g0, loc=0.0, scale=1.0)\n",
      "  (density2): Normal(name=g2, loc=6.0, scale=1.0)\n",
      "), Normal(name=g2, loc=6.0, scale=1.0)]\n"
     ]
    }
   ],
   "source": [
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ext_to=g1:NormalLinearKernel(\n",
      "  (net): LinearMap(\n",
      "    (net): Linear(in_features=1, out_features=1, bias=True)\n",
      "  )\n",
      "), ext_to=g2:NormalLinearKernel(\n",
      "  (net): LinearMap(\n",
      "    (net): Linear(in_features=1, out_features=1, bias=True)\n",
      "  )\n",
      ")]\n",
      "Parameter containing:\n",
      "tensor([[0.5153]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.4414], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1939]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.4694], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(forwards)\n",
    "\n",
    "_ = [print(p) for f in forwards  for p in f.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ext_to=g0:NormalLinearKernel(\n",
      "  (net): LinearMap(\n",
      "    (net): Linear(in_features=1, out_features=1, bias=True)\n",
      "  )\n",
      "), ext_to=g1:NormalLinearKernel(\n",
      "  (net): LinearMap(\n",
      "    (net): Linear(in_features=1, out_features=1, bias=True)\n",
      "  )\n",
      ")]\n",
      "Parameter containing:\n",
      "tensor([[-0.9414]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.5997], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.2057]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.5087], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(reverses)\n",
    "\n",
    "_ = [print(p) for f in reverses for p in f.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45907d7d079b48879c292c5c2f299979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from combinators.objectives import mb0, mb1, _estimate_mc, eval_nrep\n",
    "optimizer = torch.optim.Adam([dict(params=x.parameters()) for x in [*forwards, *reverses]], lr=1e-3)\n",
    "num_iterations=2000\n",
    "\n",
    "with trange(num_iterations) as bar:\n",
    "    for i in bar:\n",
    "        q0 = targets[0]\n",
    "        p_prv_tr, out0 = q0(sample_shape=sample_shape)\n",
    "\n",
    "        loss = torch.zeros(1, **kw_autodevice())\n",
    "        lw, lvss = torch.zeros(sample_shape, **kw_autodevice()), []\n",
    "        for k, (fwd, rev, q, p) in enumerate(zip(forwards, reverses, targets[:-1], targets[1:])):\n",
    "            q.with_observations(trace_utils.copytrace(p_prv_tr, detach=p_prv_tr.keys()))\n",
    "            q_ext = Forward(fwd, q, _step=k)\n",
    "            p_ext = Reverse(p, rev, _step=k)\n",
    "            extend = Propose(target=p_ext, proposal=q_ext, _step=k)\n",
    "#             breakpoint()\n",
    "            state, lv = extend(sample_shape=sample_shape, sample_dims=0)\n",
    "\n",
    "            p_prv_tr = state.target.trace\n",
    "            p.clear_observations()\n",
    "            q.clear_observations()\n",
    "            lw += lv\n",
    "#             loss += nvo_avo(lv)\n",
    "\n",
    "# # # # #             breakpoint()s\n",
    "#             batch_dim=None\n",
    "#             sample_dims=0\n",
    "#             rv_proposal=state.proposal.trace[f'g{k}']\n",
    "#             rv_target=state.target.trace[f'g{k+1}']\n",
    "#             # TODO: move back from the proposal and target RVs to joint logprobs?\n",
    "#             reducedims = (sample_dims,)\n",
    "\n",
    "#             lw = lw.detach()\n",
    "#             ldZ = lv.detach().logsumexp(dim=sample_dims) - math.log(lv.shape[sample_dims])\n",
    "#             f = -lv\n",
    "\n",
    "#             # rv_proposal = next(iter(proposal_trace.values())) # tr[\\gamma_{k-1}]\n",
    "#             # rv_target = next(iter(target_trace.values()))     # tr[\\gamma_{k}]\n",
    "\n",
    "#             kwargs = dict(\n",
    "#                 sample_dims=sample_dims,\n",
    "#                 reducedims=reducedims,\n",
    "#                 keepdims=False\n",
    "#             )\n",
    "\n",
    "#             baseline = _estimate_mc(f.detach(), lw, **kwargs).detach()\n",
    "\n",
    "#             kl_term = _estimate_mc(mb1(rv_proposal.log_prob.squeeze()) * (f - baseline), lw, **kwargs)\n",
    "\n",
    "#             grad_log_Z1 = _estimate_mc(rv_proposal.log_prob.squeeze(), lw, **kwargs)\n",
    "#             grad_log_Z2 = _estimate_mc(eval_nrep(rv_target).log_prob.squeeze(), lw+lv.detach(), **kwargs)\n",
    "#            #s breakpoint()\n",
    "\n",
    "            if k==0:\n",
    "#                 loss += kl_term + mb0(baseline * grad_log_Z1 - grad_log_Z2) + baseline + ldZ\n",
    "                loss += nvo_avo(lv)\n",
    "#                 loss += nvo_rkl(lw, slv, state.proposal.trace[f'g{k}'], state.target.trace[f'g{k+1}'])\n",
    "\n",
    "            lvss.append(lv)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "#         scheduler.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # REPORTING\n",
    "            # ---------------------------------------\n",
    "#             # ESS\n",
    "            lvs = torch.stack(lvss, dim=0)\n",
    "            lws = torch.cumsum(lvs, dim=1)\n",
    "            ess = effective_sample_size(lws, sample_dims=-1)\n",
    "            for step, x in zip(range(1,len(ess)+1), ess):\n",
    "                writer.add_scalar(f'ess/step-{step}', x, i)\n",
    "\n",
    "            # logZhat\n",
    "            lzh = log_Z_hat(lws, sample_dims=-1)\n",
    "            for step, x in zip(range(1,len(lzh)+1), lzh):\n",
    "                writer.add_scalar(f'log_Z_hat/step-{step}', x, i)\n",
    "\n",
    "            # loss\n",
    "            loss_ct += 1\n",
    "            loss_scalar = loss.detach().cpu().mean().item()\n",
    "            writer.add_scalar('loss', loss_scalar, i)\n",
    "            loss_sum += loss_scalar\n",
    "\n",
    "            # progress bar\n",
    "            if i % 10 == 0:\n",
    "                loss_avg = loss_sum / loss_ct\n",
    "                loss_template = 'loss={}{:.4f}'.format('' if loss_avg < 0 else ' ', loss_avg)\n",
    "                logZh_template = 'logZhat[-1]={:.4f}'.format(lzh[-1].cpu().item())\n",
    "                ess_template = 'ess[-1]={:.4f}'.format(ess[-1].cpu().item())\n",
    "                loss_ct, loss_sum  = 0, 0.0\n",
    "                bar.set_postfix_str(\"; \".join([loss_template, ess_template, logZh_template]))\n",
    "\n",
    "            # show samples\n",
    "#             if i % (eval_break + 1) == 0:\n",
    "#                 samples = sample_along(targets[0], forwards)\n",
    "#                 fig = V.scatter_along(samples)\n",
    "#                 writer.add_figure('overview', fig, global_step=i, close=True)\n",
    "# #                 for ix, xs in enumerate(samples):\n",
    "# #                     writer.add_figure(f'step-{ix+1}', V.scatter(xs), global_step=i, close=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0146;  -0.4183;  0.5374\n"
     ]
    }
   ],
   "source": [
    "samples = sample_along(targets[0], forwards)\n",
    "plot_type = len(samples[0].squeeze().shape)\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from scipy.interpolate import interpn\n",
    "from matplotlib import cm\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "\n",
    "\n",
    "def scatter(xs, lws=None, c='C0', ax=None, show=False):\n",
    "    xs = xs.squeeze().detach().cpu().numpy()\n",
    "    assert len(xs.shape) == 2\n",
    "    inplace = ax is not None\n",
    "    cm_endpoints = [(i, (*colors.to_rgb(c), i)) for i in [0.0, 1.0]]\n",
    "    lin_alpha = colors.LinearSegmentedColormap.from_list('incr_alpha', cm_endpoints)\n",
    "    fig = None\n",
    "#     if ax is None:\n",
    "#         fig, ax = plt.subplots()\n",
    "\n",
    "    plt.scatter(*xs.T, c=None if lws is None else lws.softmax(dim=0), cmap=lin_alpha)\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "    return fig if fig is not None else ax\n",
    "\n",
    "def scatter_together(samples):\n",
    "    fig = plt.figure(figsize=(5*len(samples), 5))\n",
    "    gspec = gridspec.GridSpec(ncols=len(samples), nrows=1, figure=fig)\n",
    "\n",
    "    for i, xs in enumerate(samples):\n",
    "        ax = fig.add_subplot(gspec[0, i])\n",
    "        scatter(xs)\n",
    "    return fig\n",
    "\n",
    "if plot_type == 1:\n",
    "    print(\";  \".join([\"{:.4f}\".format(ss.mean().cpu().item()) for ss in samples]))\n",
    "elif plot_type == 2:\n",
    "    fig = scatter_together(samples)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ext_to=g1:NormalLinearKernel(\n",
      "  (net): LinearMap(\n",
      "    (net): Linear(in_features=1, out_features=1, bias=True)\n",
      "  )\n",
      "), ext_to=g2:NormalLinearKernel(\n",
      "  (net): LinearMap(\n",
      "    (net): Linear(in_features=1, out_features=1, bias=True)\n",
      "  )\n",
      ")]\n",
      "Parameter containing:\n",
      "tensor([[0.5153]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.4414], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1939]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.4694], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(forwards)\n",
    "\n",
    "_ = [print(p) for f in forwards  for p in f.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ext_to=g0:NormalLinearKernel(\n",
      "  (net): LinearMap(\n",
      "    (net): Linear(in_features=1, out_features=1, bias=True)\n",
      "  )\n",
      "), ext_to=g1:NormalLinearKernel(\n",
      "  (net): LinearMap(\n",
      "    (net): Linear(in_features=1, out_features=1, bias=True)\n",
      "  )\n",
      ")]\n",
      "Parameter containing:\n",
      "tensor([[0.2448]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.0670], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.2057]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.5087], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(reverses)\n",
    "\n",
    "_ = [print(p) for f in reverses for p in f.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
