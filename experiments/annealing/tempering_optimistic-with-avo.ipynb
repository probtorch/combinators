{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nix/store/1fw9h2capcs2dpc7pgc39pn3g0zwm078-python3-3.8.6-env/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'targets': [[g0]MultivariateNormal(),\n",
       "  [β=0.3333][g1]Tempered(\n",
       "    (density1): [g0]MultivariateNormal()\n",
       "    (density2): [g3]RingGMM()\n",
       "  ),\n",
       "  [β=0.6667][g2]Tempered(\n",
       "    (density1): [g0]MultivariateNormal()\n",
       "    (density2): [g3]RingGMM()\n",
       "  ),\n",
       "  [g3]RingGMM()],\n",
       " 'forwards': [MultivariateNormalKernel(\n",
       "    (net): ResMLPJ(\n",
       "      (joint): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (mu): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=2, bias=True)\n",
       "      )\n",
       "      (cov): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=2, bias=True)\n",
       "      )\n",
       "    )\n",
       "  ),\n",
       "  MultivariateNormalKernel(\n",
       "    (net): ResMLPJ(\n",
       "      (joint): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (mu): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=2, bias=True)\n",
       "      )\n",
       "      (cov): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=2, bias=True)\n",
       "      )\n",
       "    )\n",
       "  ),\n",
       "  MultivariateNormalKernel(\n",
       "    (net): ResMLPJ(\n",
       "      (joint): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (mu): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=2, bias=True)\n",
       "      )\n",
       "      (cov): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=2, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )],\n",
       " 'reverses': [MultivariateNormalKernel(\n",
       "    (net): ResMLPJ(\n",
       "      (joint): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (mu): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=2, bias=True)\n",
       "      )\n",
       "      (cov): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=2, bias=True)\n",
       "      )\n",
       "    )\n",
       "  ),\n",
       "  MultivariateNormalKernel(\n",
       "    (net): ResMLPJ(\n",
       "      (joint): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (mu): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=2, bias=True)\n",
       "      )\n",
       "      (cov): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=2, bias=True)\n",
       "      )\n",
       "    )\n",
       "  ),\n",
       "  MultivariateNormalKernel(\n",
       "    (net): ResMLPJ(\n",
       "      (joint): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (mu): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=2, bias=True)\n",
       "      )\n",
       "      (cov): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=2, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )]}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import torch\n",
    "import math\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import trange\n",
    "from typing import Tuple\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import combinators.trace.utils as trace_utils\n",
    "from combinators.trace.utils import RequiresGrad\n",
    "from combinators.tensor.utils import autodevice, kw_autodevice, copy, show\n",
    "from combinators.densities import MultivariateNormal, Tempered, RingGMM, Normal\n",
    "from combinators.densities.kernels import MultivariateNormalKernel, MultivariateNormalLinearKernel, NormalLinearKernel\n",
    "from combinators.nnets import ResMLPJ\n",
    "from combinators.objectives import nvo_rkl, nvo_avo\n",
    "from combinators import Forward, Reverse, Propose\n",
    "from combinators.stochastic import RandomVariable, ImproperRandomVariable\n",
    "from combinators.metrics import effective_sample_size, log_Z_hat\n",
    "import visualize as V\n",
    "\n",
    "def mk_kernel(from_:int, to_:int, std:float, num_hidden:int, learn_cov=True):\n",
    "    embedding_dim = 2\n",
    "    return MultivariateNormalKernel(\n",
    "        ext_from=f'g{from_}',\n",
    "        ext_to=f'g{to_}',\n",
    "        loc=torch.zeros(2, **kw_autodevice()),\n",
    "        cov=torch.eye(2, **kw_autodevice())*std**2,\n",
    "        learn_cov=learn_cov,\n",
    "        net=ResMLPJ(\n",
    "            dim_in=2,\n",
    "            dim_hidden=num_hidden,\n",
    "            dim_out=embedding_dim).to(autodevice()))\n",
    "\n",
    "def mk_mnlinear_kernel(from_:int, to_:int, std:float, dim:int):\n",
    "    return MultivariateNormalLinearKernel(\n",
    "        ext_from=f'g{from_}',\n",
    "        ext_to=f'g{to_}',\n",
    "        loc=torch.zeros(dim, **kw_autodevice()),\n",
    "        cov=torch.eye(dim, **kw_autodevice())*std**2)\n",
    "\n",
    "def mk_nlinear_kernel(from_:int, to_:int, std:float, dim:int):\n",
    "    return NormalLinearKernel(ext_from=f'g{from_}', ext_to=f'g{to_}')\n",
    "\n",
    "def anneal_to_ring(num_targets):\n",
    "    proposal_std = 5\n",
    "    g0 = mk_mvn(0, 0, std=proposal_std)\n",
    "    gK = RingGMM(loc_scale=3, scale=0.16, count=2, name=f\"g{num_targets - 1}\").to(autodevice())\n",
    "    return anneal_between(g0, gK, num_targets)\n",
    "\n",
    "def anneal_between(left, right, total_num_targets):\n",
    "    proposal_std = total_num_targets\n",
    "\n",
    "    # Make an annealing path\n",
    "    betas = torch.arange(0., 1., 1./(total_num_targets - 1))[1:] # g_0 is beta=0\n",
    "    path = [Tempered(f'g{k}', left, right, beta) for k, beta in zip(range(1,total_num_targets-1), betas)]\n",
    "    path = [left] + path + [right]\n",
    "    \n",
    "    assert len(path) == total_num_targets # sanity check that the betas line up\n",
    "    return path\n",
    "\n",
    "\n",
    "def anneal_between_mvns(left_loc, right_loc, total_num_targets):\n",
    "    g0 = mk_mvn(0, left_loc)\n",
    "    gK =  mk_mvn(total_num_targets-1, right_loc)\n",
    "\n",
    "    return anneal_between(g0, gK, total_num_targets)\n",
    "\n",
    "def anneal_between_ns(left_loc, right_loc, total_num_targets):\n",
    "    g0 = mk_n(0, left_loc)\n",
    "    gK =  mk_n(total_num_targets-1, right_loc)\n",
    "\n",
    "    return anneal_between(g0, gK, total_num_targets)\n",
    "\n",
    "def mk_mvn(i, loc, std=1):\n",
    "    return MultivariateNormal(name=f'g{i}', loc=torch.ones(2, **kw_autodevice())*loc, cov=torch.eye(2, **kw_autodevice())*std**2)\n",
    "\n",
    "def mk_n(i, loc):\n",
    "    return Normal(name=f'g{i}', loc=torch.ones(1, **kw_autodevice())*loc, scale=torch.ones(1, **kw_autodevice())**2)\n",
    "\n",
    "def mk_model(num_targets:int):\n",
    "    return dict(\n",
    "        targets=anneal_to_ring(num_targets),\n",
    "        forwards=[mk_kernel(from_=i, to_=i+1, std=1., num_hidden=64) for i in range(num_targets-1)],\n",
    "        reverses=[mk_kernel(from_=i+1, to_=i, std=1., num_hidden=64) for i in range(num_targets-1)],\n",
    "\n",
    "#         targets=anneal_between_mvns(0, num_targets*2, num_targets),\n",
    "#         forwards=[mk_kernel(from_=i, to_=i+1, std=1., num_hidden=64) for i in range(num_targets-1)],\n",
    "#         reverses=[mk_kernel(from_=i+1, to_=i, std=1., num_hidden=64) for i in range(num_targets-1)],\n",
    "\n",
    "#         targets=anneal_between_mvns(0, num_targets*2, num_targets),\n",
    "#         forwards=[mk_mnlinear_kernel(from_=i, to_=i+1, std=1., dim=2) for i in range(num_targets-1)],\n",
    "#         reverses=[mk_mnlinear_kernel(from_=i+1, to_=i, std=1., dim=2) for i in range(num_targets-1)],\n",
    "\n",
    "        # NOTES: Anneal between 2 1d guassians with a linear kernel: 2 steps\n",
    "        # annealing does not learn the forward kernel in the first step, but learns both in the second step. \n",
    "#         targets=anneal_between_ns(0, num_targets*2, num_targets),\n",
    "#         forwards=[mk_nlinear_kernel(from_=i, to_=i+1, std=1., dim=1) for i in range(num_targets-1)],\n",
    "#         reverses=[mk_nlinear_kernel(from_=i+1, to_=i, std=1., dim=1) for i in range(num_targets-1)],\n",
    "\n",
    "#         targets=[mk_mvn(i, i*2) for i in range(num_targets)],\n",
    "#         forwards=[mk_kernel(from_=i, to_=i+1, std=1., num_hidden=32) for i in range(num_targets-1)],\n",
    "#         reverses=[mk_kernel(from_=i+1, to_=i, std=1., num_hidden=32) for i in range(num_targets-1)],\n",
    "\n",
    "#         targets=[mk_mvn(i, i*2) for i in range(num_targets)],\n",
    "#         forwards=[mk_mnlinear_kernel(from_=i, to_=i+1, std=1., dim=2) for i in range(num_targets-1)],\n",
    "#         reverses=[mk_mnlinear_kernel(from_=i+1, to_=i, std=1., dim=2) for i in range(num_targets-1)],\n",
    "\n",
    "        # NOTES: With 1 intermediate density between 2 1d guassians with a linear kernel everything is fine\n",
    "#         targets=[mk_n(i, i*2) for i in range(num_targets)],\n",
    "#         forwards=[mk_nlinear_kernel(from_=i, to_=i+1, std=1., dim=1) for i in range(num_targets-1)],\n",
    "#         reverses=[mk_nlinear_kernel(from_=i+1, to_=i, std=1., dim=1) for i in range(num_targets-1)],\n",
    "    )\n",
    "K = 4\n",
    "\n",
    "mk_model(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import trange\n",
    "from typing import Tuple\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import combinators.trace.utils as trace_utils\n",
    "from combinators.tensor.utils import autodevice, kw_autodevice\n",
    "from combinators.densities import MultivariateNormal, Tempered, RingGMM\n",
    "from combinators.densities.kernels import MultivariateNormalKernel\n",
    "from combinators.nnets import ResMLPJ\n",
    "from combinators.objectives import nvo_rkl\n",
    "from combinators import Forward, Reverse, Propose\n",
    "from combinators.stochastic import RandomVariable, ImproperRandomVariable\n",
    "from combinators.metrics import effective_sample_size, log_Z_hat\n",
    "import visualize as V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from main import mk_model, mk_kernel\n",
    "from tqdm.notebook import trange, tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from combinators import Forward\n",
    "\n",
    "def sample_along(proposal, kernels, sample_shape=(2000,)):\n",
    "    samples = []\n",
    "    tr, out = proposal(sample_shape=sample_shape)\n",
    "    samples.append(out)\n",
    "    for k in forwards:\n",
    "        proposal = Forward(k, proposal)\n",
    "        tr, out = proposal(sample_shape=sample_shape)\n",
    "        samples.append(out)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main() arguments\n",
    "seed=1\n",
    "eval_break = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "torch.manual_seed(seed)\n",
    "num_samples = 256\n",
    "sample_shape=(num_samples,)\n",
    "\n",
    "# Models\n",
    "out = mk_model(K)\n",
    "targets, forwards, reverses = [[m.to(autodevice()) for m in out[n]] for n in ['targets', 'forwards', 'reverses']]\n",
    "\n",
    "assert all([len(list(k.parameters())) >  0 for k in [*forwards, *reverses]])\n",
    "\n",
    "# logging\n",
    "writer = SummaryWriter()\n",
    "loss_ct, loss_sum, loss_avgs, loss_all = 0, 0.0, [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[g0]MultivariateNormal(), [β=0.3333][g1]Tempered(\n",
      "  (density1): [g0]MultivariateNormal()\n",
      "  (density2): [g3]RingGMM()\n",
      "), [β=0.6667][g2]Tempered(\n",
      "  (density1): [g0]MultivariateNormal()\n",
      "  (density2): [g3]RingGMM()\n",
      "), [g3]RingGMM()]\n"
     ]
    }
   ],
   "source": [
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MultivariateNormalKernel(\n",
      "  (net): ResMLPJ(\n",
      "    (joint): Sequential(\n",
      "      (0): Linear(in_features=2, out_features=64, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (mu): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=2, bias=True)\n",
      "    )\n",
      "    (cov): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      "), MultivariateNormalKernel(\n",
      "  (net): ResMLPJ(\n",
      "    (joint): Sequential(\n",
      "      (0): Linear(in_features=2, out_features=64, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (mu): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=2, bias=True)\n",
      "    )\n",
      "    (cov): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      "), MultivariateNormalKernel(\n",
      "  (net): ResMLPJ(\n",
      "    (joint): Sequential(\n",
      "      (0): Linear(in_features=2, out_features=64, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (mu): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=2, bias=True)\n",
      "    )\n",
      "    (cov): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")]\n",
      "Parameter containing:\n",
      "tensor([0.5413, 0.5413], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.3643, -0.3121],\n",
      "        [-0.1371,  0.3319],\n",
      "        [-0.6657,  0.4241],\n",
      "        [-0.1455,  0.3597],\n",
      "        [ 0.0983, -0.0866],\n",
      "        [ 0.1961,  0.0349],\n",
      "        [ 0.2583, -0.2756],\n",
      "        [-0.0516, -0.0637],\n",
      "        [ 0.1025, -0.0028],\n",
      "        [ 0.6181,  0.2200],\n",
      "        [-0.2633, -0.4271],\n",
      "        [-0.1185, -0.3050],\n",
      "        [-0.2266,  0.0339],\n",
      "        [ 0.4215,  0.3843],\n",
      "        [-0.6912,  0.4383],\n",
      "        [ 0.1975,  0.6707],\n",
      "        [ 0.4667, -0.6443],\n",
      "        [-0.6723, -0.3411],\n",
      "        [ 0.6209, -0.1178],\n",
      "        [ 0.3026, -0.3286],\n",
      "        [ 0.6938, -0.2992],\n",
      "        [ 0.5303,  0.0084],\n",
      "        [-0.3725,  0.3635],\n",
      "        [-0.3753,  0.2080],\n",
      "        [-0.2042, -0.0775],\n",
      "        [-0.6798, -0.3371],\n",
      "        [ 0.3837, -0.1719],\n",
      "        [ 0.7043,  0.5668],\n",
      "        [-0.0331, -0.4720],\n",
      "        [ 0.4306,  0.2195],\n",
      "        [-0.4571,  0.4593],\n",
      "        [ 0.4293,  0.6271],\n",
      "        [-0.3964, -0.1164],\n",
      "        [-0.0137,  0.1033],\n",
      "        [-0.5366, -0.5018],\n",
      "        [ 0.3847, -0.1658],\n",
      "        [ 0.3454,  0.0403],\n",
      "        [ 0.2322,  0.1555],\n",
      "        [ 0.2571,  0.3505],\n",
      "        [-0.6549,  0.3559],\n",
      "        [-0.4972, -0.5335],\n",
      "        [ 0.0430, -0.1205],\n",
      "        [ 0.4153, -0.4095],\n",
      "        [-0.6286,  0.5146],\n",
      "        [-0.1049,  0.3977],\n",
      "        [ 0.2273, -0.5302],\n",
      "        [ 0.1421,  0.1698],\n",
      "        [-0.4734, -0.3355],\n",
      "        [ 0.2411,  0.1267],\n",
      "        [-0.3008, -0.2141],\n",
      "        [ 0.6476, -0.1308],\n",
      "        [ 0.3987,  0.3062],\n",
      "        [-0.4571, -0.6013],\n",
      "        [ 0.6787,  0.0369],\n",
      "        [ 0.4847,  0.1465],\n",
      "        [ 0.2274,  0.5282],\n",
      "        [ 0.6705, -0.4692],\n",
      "        [ 0.0884,  0.5277],\n",
      "        [ 0.5123,  0.4393],\n",
      "        [-0.5117, -0.5092],\n",
      "        [-0.4276,  0.0888],\n",
      "        [ 0.7047, -0.4467],\n",
      "        [ 0.3768, -0.3914],\n",
      "        [-0.6648, -0.1503]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.4074,  0.6565, -0.4391,  0.1535,  0.6101,  0.4686,  0.4407,  0.5025,\n",
      "         0.4473,  0.1826, -0.4835, -0.5938, -0.3240, -0.0823, -0.4334,  0.2587,\n",
      "         0.2188, -0.1601,  0.2718,  0.2285,  0.4317,  0.4762, -0.2395,  0.6909,\n",
      "        -0.0817, -0.0243, -0.6674, -0.4551, -0.4131, -0.3024,  0.5027, -0.2311,\n",
      "        -0.5284,  0.2721,  0.2264,  0.4580, -0.3659,  0.1533, -0.2574, -0.1589,\n",
      "        -0.5635, -0.3223, -0.2166,  0.3024,  0.1292,  0.1747,  0.7058,  0.6892,\n",
      "         0.4823,  0.0225, -0.4892,  0.5526, -0.1768, -0.0572, -0.6092, -0.1397,\n",
      "        -0.4561,  0.6498, -0.6113, -0.5512, -0.0240, -0.3823,  0.2530, -0.2722],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0587,  0.0071,  0.0905, -0.0879,  0.0587,  0.0803,  0.1223, -0.0875,\n",
      "          0.0303, -0.0924,  0.1067, -0.0485,  0.0753,  0.0037, -0.0097, -0.0040,\n",
      "          0.0212,  0.0589,  0.0200,  0.0381, -0.1124,  0.0911,  0.1090,  0.1033,\n",
      "          0.0924, -0.0902, -0.0463,  0.1102, -0.0952,  0.1134, -0.0983, -0.0881,\n",
      "          0.0611, -0.0898, -0.0286,  0.0909,  0.0990,  0.1182, -0.0254, -0.0972,\n",
      "          0.1231, -0.0266, -0.0514,  0.0305, -0.0874,  0.0822,  0.0783, -0.0992,\n",
      "         -0.1027, -0.0109,  0.0525, -0.0036, -0.0634,  0.0029, -0.1175, -0.0883,\n",
      "         -0.0832,  0.1029,  0.1102, -0.0425,  0.0056,  0.0557,  0.0150, -0.0626],\n",
      "        [ 0.0721,  0.0768, -0.0072, -0.0154,  0.1136,  0.1093, -0.0709,  0.1223,\n",
      "          0.0309, -0.0830,  0.0684, -0.0933,  0.1155, -0.0804,  0.0354,  0.0381,\n",
      "          0.0297,  0.1037, -0.0519, -0.0528, -0.1083, -0.0051, -0.0592,  0.0050,\n",
      "         -0.0256,  0.0415,  0.1081,  0.0369, -0.0403, -0.0613, -0.1090,  0.1052,\n",
      "         -0.0237,  0.0252,  0.0046, -0.0797,  0.0704,  0.0701,  0.0063, -0.0710,\n",
      "         -0.0531,  0.0017, -0.0714, -0.0697, -0.0186, -0.0360,  0.0307, -0.0329,\n",
      "         -0.0166, -0.0477, -0.1142,  0.1090,  0.0238,  0.1118, -0.0201, -0.0314,\n",
      "         -0.1175, -0.0475,  0.0820, -0.1224, -0.0437,  0.0762,  0.0720,  0.0462]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0247, -0.0095], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.5413, 0.5413], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.7009, -0.0739],\n",
      "        [ 0.6802,  0.4543],\n",
      "        [-0.3507, -0.5454],\n",
      "        [ 0.3966, -0.3874],\n",
      "        [ 0.3243, -0.6056],\n",
      "        [ 0.5159,  0.2119],\n",
      "        [-0.0813,  0.3243],\n",
      "        [-0.5034, -0.1546],\n",
      "        [ 0.0069, -0.4086],\n",
      "        [-0.1807, -0.3049],\n",
      "        [ 0.6739,  0.6007],\n",
      "        [ 0.6099,  0.5779],\n",
      "        [-0.6772,  0.2365],\n",
      "        [ 0.2606, -0.5186],\n",
      "        [ 0.2866, -0.3837],\n",
      "        [ 0.1124,  0.2342],\n",
      "        [ 0.6102, -0.3710],\n",
      "        [-0.5563,  0.6543],\n",
      "        [ 0.2968,  0.3437],\n",
      "        [ 0.2545, -0.2503],\n",
      "        [ 0.4254, -0.6173],\n",
      "        [ 0.0984,  0.3196],\n",
      "        [ 0.4831, -0.3531],\n",
      "        [-0.4779,  0.2650],\n",
      "        [-0.5616, -0.5877],\n",
      "        [-0.6359, -0.3429],\n",
      "        [-0.1350, -0.4187],\n",
      "        [-0.2990, -0.6261],\n",
      "        [ 0.4346, -0.1619],\n",
      "        [-0.4461,  0.1841],\n",
      "        [-0.5287,  0.4950],\n",
      "        [ 0.4066, -0.6949],\n",
      "        [-0.5550, -0.4662],\n",
      "        [-0.3828,  0.0883],\n",
      "        [-0.6992, -0.5242],\n",
      "        [ 0.3229, -0.2316],\n",
      "        [-0.2814, -0.5850],\n",
      "        [ 0.2866,  0.0499],\n",
      "        [-0.2733, -0.4330],\n",
      "        [ 0.1504,  0.1924],\n",
      "        [ 0.3139,  0.2101],\n",
      "        [-0.1079, -0.5474],\n",
      "        [ 0.4502, -0.0061],\n",
      "        [-0.4699,  0.6098],\n",
      "        [ 0.3942, -0.5557],\n",
      "        [-0.3531, -0.2406],\n",
      "        [-0.4082,  0.3302],\n",
      "        [-0.5044,  0.6572],\n",
      "        [-0.2923,  0.4174],\n",
      "        [ 0.0241, -0.3110],\n",
      "        [ 0.4722, -0.5395],\n",
      "        [-0.3741,  0.0848],\n",
      "        [ 0.5609, -0.3030],\n",
      "        [-0.4306, -0.4514],\n",
      "        [-0.3117, -0.2443],\n",
      "        [-0.1647, -0.4023],\n",
      "        [ 0.2210,  0.0057],\n",
      "        [-0.4620, -0.4038],\n",
      "        [ 0.1497, -0.0101],\n",
      "        [ 0.5005, -0.1072],\n",
      "        [-0.5728, -0.5230],\n",
      "        [-0.2076, -0.1566],\n",
      "        [ 0.0807, -0.1586],\n",
      "        [ 0.2609, -0.2352]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.5190,  0.1624, -0.2174,  0.6302,  0.1805, -0.4805, -0.3935,  0.4434,\n",
      "         0.3230,  0.5521, -0.0543,  0.4946,  0.3516,  0.2187, -0.1637,  0.6816,\n",
      "         0.1431, -0.1824, -0.0101,  0.6951,  0.4749, -0.0524,  0.6932,  0.3105,\n",
      "        -0.3765, -0.6435,  0.4109,  0.6631, -0.4120,  0.2904,  0.0301, -0.1512,\n",
      "        -0.1923,  0.1967,  0.1559,  0.4776,  0.2589,  0.3218,  0.6238,  0.2202,\n",
      "         0.1050, -0.2574,  0.5186,  0.1044,  0.4558,  0.6828, -0.1581,  0.4979,\n",
      "        -0.1436,  0.6381,  0.5566, -0.2535,  0.0254, -0.3088, -0.2860, -0.1432,\n",
      "        -0.0048, -0.2465, -0.6675, -0.4277, -0.0604, -0.2257,  0.5252,  0.5748],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0796,  0.0553, -0.0991,  0.1181, -0.1102, -0.0411, -0.0880,  0.0450,\n",
      "         -0.1040, -0.0616, -0.0794, -0.0768, -0.0584,  0.1229, -0.0102,  0.0381,\n",
      "         -0.1241,  0.1111, -0.0272, -0.1147,  0.0861, -0.0338,  0.1135, -0.0849,\n",
      "         -0.1161, -0.0939,  0.0541, -0.0140, -0.1158,  0.0232, -0.0080,  0.0452,\n",
      "         -0.0636, -0.0892,  0.0852, -0.0602,  0.0879,  0.1014, -0.0874, -0.0576,\n",
      "          0.0150, -0.1179,  0.0384, -0.0521, -0.0554, -0.0966,  0.0585, -0.0797,\n",
      "         -0.0999, -0.0329, -0.0696, -0.1215,  0.0504, -0.0476,  0.0876,  0.0880,\n",
      "         -0.0797, -0.0729, -0.0444,  0.0732, -0.0043, -0.0447,  0.0774,  0.1185],\n",
      "        [ 0.0924, -0.0956, -0.0260, -0.0240,  0.1067, -0.0647, -0.0716,  0.0020,\n",
      "         -0.0290,  0.0473,  0.0152, -0.0746,  0.0909,  0.0778,  0.0274, -0.1131,\n",
      "          0.0587,  0.0831,  0.0348, -0.0483,  0.0084, -0.0480,  0.1177,  0.1207,\n",
      "         -0.0146, -0.0841,  0.0735,  0.0638,  0.0663, -0.0941,  0.0424, -0.0533,\n",
      "         -0.1099, -0.0873,  0.0530,  0.0468, -0.1097,  0.0709,  0.0958,  0.0949,\n",
      "          0.0706, -0.0196,  0.0838,  0.0979,  0.0652, -0.0899, -0.0436,  0.0990,\n",
      "          0.0953,  0.0798, -0.0522, -0.0248, -0.0931, -0.0712,  0.0960, -0.1026,\n",
      "         -0.0309,  0.0265, -0.0750,  0.0291, -0.0201,  0.0500, -0.0959,  0.0173]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1140,  0.0775], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.5413, 0.5413], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.5476, -0.5725],\n",
      "        [-0.1011, -0.0501],\n",
      "        [-0.6401,  0.4275],\n",
      "        [-0.2250,  0.3177],\n",
      "        [ 0.3946,  0.6235],\n",
      "        [ 0.1749,  0.5309],\n",
      "        [ 0.6530,  0.1401],\n",
      "        [-0.1441, -0.1936],\n",
      "        [ 0.1146,  0.2545],\n",
      "        [-0.6671,  0.5298],\n",
      "        [ 0.1115, -0.6587],\n",
      "        [ 0.2265,  0.4487],\n",
      "        [ 0.2642,  0.5772],\n",
      "        [ 0.0146, -0.5866],\n",
      "        [-0.0016,  0.5140],\n",
      "        [-0.3401,  0.3016],\n",
      "        [ 0.1630, -0.1147],\n",
      "        [ 0.0050,  0.3008],\n",
      "        [-0.4839,  0.4465],\n",
      "        [ 0.4787,  0.3968],\n",
      "        [ 0.7066, -0.0309],\n",
      "        [-0.4993,  0.0323],\n",
      "        [-0.2051,  0.2496],\n",
      "        [ 0.5981, -0.3870],\n",
      "        [ 0.0296,  0.6878],\n",
      "        [-0.5645, -0.1220],\n",
      "        [ 0.6866,  0.4941],\n",
      "        [ 0.2052, -0.3050],\n",
      "        [-0.0659, -0.6341],\n",
      "        [-0.6657, -0.6625],\n",
      "        [ 0.6792,  0.5593],\n",
      "        [ 0.2805, -0.1776],\n",
      "        [ 0.6088, -0.1868],\n",
      "        [ 0.4909, -0.0230],\n",
      "        [-0.3326, -0.1974],\n",
      "        [-0.1510, -0.6472],\n",
      "        [ 0.2115, -0.2154],\n",
      "        [ 0.3249,  0.4508],\n",
      "        [-0.1124,  0.3429],\n",
      "        [ 0.1413,  0.4749],\n",
      "        [-0.4560, -0.0621],\n",
      "        [-0.2356,  0.6196],\n",
      "        [-0.6347, -0.3974],\n",
      "        [ 0.0798,  0.6277],\n",
      "        [ 0.4784, -0.6617],\n",
      "        [-0.4971,  0.6856],\n",
      "        [ 0.1031, -0.6056],\n",
      "        [-0.2228,  0.3091],\n",
      "        [-0.1479, -0.5431],\n",
      "        [-0.4366, -0.3663],\n",
      "        [-0.0277,  0.2771],\n",
      "        [ 0.3395, -0.3435],\n",
      "        [-0.1186, -0.5459],\n",
      "        [ 0.5347,  0.7057],\n",
      "        [ 0.6873,  0.2890],\n",
      "        [-0.1508, -0.0176],\n",
      "        [ 0.2551, -0.6129],\n",
      "        [ 0.0439, -0.5748],\n",
      "        [-0.7070, -0.0236],\n",
      "        [-0.2251, -0.4479],\n",
      "        [ 0.1172,  0.0288],\n",
      "        [ 0.1079, -0.3958],\n",
      "        [-0.6926,  0.0155],\n",
      "        [-0.2885, -0.2384]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.5206,  0.1155,  0.3229, -0.1976, -0.5611,  0.1410, -0.4683,  0.0876,\n",
      "        -0.5518,  0.5331, -0.4817,  0.6139,  0.4995,  0.5345,  0.4243,  0.1637,\n",
      "        -0.0345, -0.0113, -0.2812, -0.6500,  0.6445,  0.0366,  0.3326,  0.0260,\n",
      "         0.2032, -0.4357, -0.5497,  0.5393,  0.2681,  0.3491, -0.7018,  0.0478,\n",
      "        -0.6596, -0.0809, -0.0374, -0.4975, -0.7002,  0.0047,  0.6359,  0.6373,\n",
      "        -0.4305, -0.1256, -0.5913, -0.5522, -0.1939,  0.6865, -0.1845,  0.0409,\n",
      "         0.6652,  0.1260,  0.5704,  0.4699,  0.6631,  0.1213, -0.5718, -0.6083,\n",
      "        -0.3653,  0.5836,  0.4066,  0.2070, -0.5385, -0.5208,  0.5769, -0.0734],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1245, -0.0155, -0.0300,  0.0019,  0.0523,  0.0481, -0.0956, -0.0014,\n",
      "         -0.0619,  0.0846,  0.0619,  0.0686,  0.1136,  0.0614, -0.1249,  0.0315,\n",
      "         -0.0858,  0.0167,  0.0013,  0.1240, -0.0854, -0.1225, -0.0335,  0.0201,\n",
      "          0.0916,  0.1014,  0.0963,  0.0395, -0.0357, -0.0264, -0.0684,  0.0244,\n",
      "         -0.0869,  0.0430,  0.0807,  0.1061, -0.1232,  0.1026, -0.0785, -0.0102,\n",
      "          0.1141,  0.0677, -0.0397,  0.0738, -0.1216,  0.0977,  0.1213, -0.0057,\n",
      "          0.0206, -0.0293, -0.0791, -0.0565, -0.0083, -0.1023, -0.1010,  0.0137,\n",
      "         -0.0813, -0.0680, -0.0011, -0.0659,  0.1157, -0.0451, -0.0695, -0.0333],\n",
      "        [-0.1020,  0.0644,  0.1074, -0.0721, -0.0465,  0.1167, -0.0904,  0.0339,\n",
      "         -0.1018, -0.1113,  0.0002, -0.0713,  0.0677, -0.0387,  0.0012, -0.0653,\n",
      "          0.0514,  0.1158, -0.0188, -0.0772, -0.1219,  0.1198,  0.0527,  0.0516,\n",
      "          0.0213,  0.0627,  0.0444,  0.0610,  0.0782,  0.0177,  0.1077,  0.0777,\n",
      "         -0.0038, -0.0558,  0.0083,  0.0974,  0.0792, -0.0507, -0.0261,  0.0496,\n",
      "          0.0471,  0.1190,  0.0607,  0.0398,  0.0289, -0.1175, -0.0461,  0.0132,\n",
      "         -0.0870, -0.1095,  0.0843,  0.0425, -0.1009,  0.0074,  0.0736,  0.0720,\n",
      "          0.1142, -0.0494,  0.0557, -0.1014, -0.0154, -0.0795, -0.0238,  0.0712]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0623, -0.0658], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(forwards)\n",
    "\n",
    "_ = [print(p) for f in forwards  for p in f.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MultivariateNormalKernel(\n",
      "  (net): ResMLPJ(\n",
      "    (joint): Sequential(\n",
      "      (0): Linear(in_features=2, out_features=64, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (mu): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=2, bias=True)\n",
      "    )\n",
      "    (cov): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      "), MultivariateNormalKernel(\n",
      "  (net): ResMLPJ(\n",
      "    (joint): Sequential(\n",
      "      (0): Linear(in_features=2, out_features=64, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (mu): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=2, bias=True)\n",
      "    )\n",
      "    (cov): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      "), MultivariateNormalKernel(\n",
      "  (net): ResMLPJ(\n",
      "    (joint): Sequential(\n",
      "      (0): Linear(in_features=2, out_features=64, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (mu): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=2, bias=True)\n",
      "    )\n",
      "    (cov): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")]\n",
      "Parameter containing:\n",
      "tensor([0.5413, 0.5413], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.5953,  0.5548],\n",
      "        [ 0.1721,  0.1227],\n",
      "        [ 0.1214,  0.4933],\n",
      "        [-0.3220,  0.5154],\n",
      "        [ 0.6800, -0.1823],\n",
      "        [ 0.4463,  0.3901],\n",
      "        [ 0.6622, -0.1646],\n",
      "        [-0.4206, -0.2625],\n",
      "        [-0.4811,  0.0243],\n",
      "        [ 0.6818,  0.6860],\n",
      "        [ 0.5341,  0.0700],\n",
      "        [ 0.0154,  0.4682],\n",
      "        [ 0.4199, -0.0702],\n",
      "        [-0.1788,  0.2796],\n",
      "        [-0.0245, -0.4968],\n",
      "        [-0.3326,  0.5735],\n",
      "        [ 0.5889, -0.2755],\n",
      "        [-0.1517, -0.3505],\n",
      "        [-0.2309,  0.6446],\n",
      "        [-0.6289,  0.3374],\n",
      "        [-0.6355, -0.3374],\n",
      "        [-0.1440,  0.0490],\n",
      "        [-0.1148, -0.3232],\n",
      "        [-0.3154, -0.4301],\n",
      "        [-0.4936, -0.4405],\n",
      "        [ 0.6781, -0.3332],\n",
      "        [-0.5880,  0.4860],\n",
      "        [ 0.2743,  0.4864],\n",
      "        [-0.7034, -0.6731],\n",
      "        [-0.5019, -0.3725],\n",
      "        [-0.0016,  0.3059],\n",
      "        [ 0.4140, -0.6082],\n",
      "        [ 0.2820, -0.4089],\n",
      "        [ 0.1625,  0.2220],\n",
      "        [ 0.1132,  0.2502],\n",
      "        [-0.0353,  0.4668],\n",
      "        [ 0.1475,  0.6195],\n",
      "        [ 0.1533, -0.3743],\n",
      "        [ 0.4541,  0.0383],\n",
      "        [ 0.6834, -0.0782],\n",
      "        [ 0.6996, -0.4128],\n",
      "        [ 0.3554, -0.5275],\n",
      "        [ 0.4855,  0.1284],\n",
      "        [ 0.4015, -0.6089],\n",
      "        [-0.7034,  0.3407],\n",
      "        [-0.2909,  0.2800],\n",
      "        [-0.4195, -0.2186],\n",
      "        [ 0.0032,  0.4258],\n",
      "        [ 0.3993, -0.0632],\n",
      "        [-0.2000, -0.0123],\n",
      "        [ 0.3644, -0.5993],\n",
      "        [-0.2161, -0.6816],\n",
      "        [ 0.1021, -0.0836],\n",
      "        [ 0.4006,  0.5649],\n",
      "        [-0.6046, -0.3129],\n",
      "        [-0.1137, -0.0071],\n",
      "        [-0.1614, -0.4446],\n",
      "        [-0.2732, -0.0182],\n",
      "        [ 0.5686, -0.1802],\n",
      "        [ 0.4670, -0.5716],\n",
      "        [ 0.0316, -0.5518],\n",
      "        [-0.6635,  0.0701],\n",
      "        [-0.3238,  0.5206],\n",
      "        [ 0.5585, -0.7038]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0493, -0.1208, -0.4030, -0.4937, -0.2992,  0.3122,  0.1681,  0.4256,\n",
      "        -0.0422,  0.3673,  0.1367, -0.1542, -0.0413, -0.6431, -0.0162, -0.2323,\n",
      "        -0.2366, -0.2840, -0.1546, -0.5220,  0.0415, -0.0287, -0.1862,  0.6138,\n",
      "         0.4746,  0.1997, -0.2604,  0.0902, -0.4865,  0.4927,  0.6605, -0.1056,\n",
      "         0.5453,  0.6636, -0.0425, -0.2626,  0.0357,  0.1169, -0.6545, -0.0441,\n",
      "        -0.2557,  0.4718, -0.0203,  0.4652,  0.6290, -0.2216,  0.1522, -0.0605,\n",
      "        -0.3082, -0.3923,  0.0164,  0.2148, -0.1244,  0.1201,  0.4419,  0.2948,\n",
      "         0.2916,  0.1987, -0.5436, -0.4278, -0.5244,  0.4647, -0.6228, -0.5279],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0823,  0.0739, -0.1182,  0.0722, -0.0285,  0.0269, -0.0783, -0.0190,\n",
      "          0.0278,  0.0953,  0.0758, -0.0680, -0.0820, -0.0076,  0.0392,  0.0685,\n",
      "         -0.1246, -0.0102,  0.0044, -0.0584,  0.0986, -0.0332, -0.1192, -0.0746,\n",
      "          0.0040, -0.0930,  0.0286,  0.1216,  0.0051, -0.0308,  0.1178,  0.0923,\n",
      "          0.0831,  0.0095,  0.0057, -0.0969,  0.0127,  0.1216,  0.0066, -0.1115,\n",
      "          0.0686,  0.1122,  0.1048,  0.0059,  0.0235,  0.0202, -0.0512, -0.0182,\n",
      "          0.1020,  0.0128,  0.1178, -0.0830, -0.0183,  0.0015, -0.0744,  0.0965,\n",
      "          0.0873, -0.0674,  0.1101,  0.1166, -0.0735, -0.0043,  0.1246, -0.0812],\n",
      "        [-0.0196, -0.0858,  0.0902,  0.0203,  0.0520,  0.0897, -0.0198, -0.1249,\n",
      "          0.0061, -0.0097,  0.0453,  0.0583, -0.0622, -0.0492,  0.0788,  0.0356,\n",
      "          0.0286,  0.0557,  0.0581, -0.0903,  0.0100,  0.0119, -0.1207,  0.0265,\n",
      "         -0.0017,  0.0970, -0.1193,  0.1176, -0.0612,  0.0736,  0.0923, -0.0155,\n",
      "         -0.0640, -0.0208, -0.0780,  0.0155,  0.0525,  0.0258, -0.0720,  0.0683,\n",
      "          0.1147, -0.0380,  0.0649, -0.0082,  0.0504, -0.0891, -0.1067, -0.0035,\n",
      "         -0.1053,  0.0450,  0.1170, -0.0245,  0.0012,  0.0190, -0.0604, -0.0064,\n",
      "          0.0322,  0.1011, -0.1037, -0.0025, -0.1020, -0.0111, -0.0033, -0.0521]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.0767, 0.0858], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.5413, 0.5413], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0846,  0.1398],\n",
      "        [-0.2136,  0.2931],\n",
      "        [ 0.3301, -0.6468],\n",
      "        [ 0.6234, -0.5702],\n",
      "        [-0.3156,  0.2784],\n",
      "        [ 0.6517,  0.2299],\n",
      "        [-0.3946,  0.1244],\n",
      "        [-0.4244,  0.3327],\n",
      "        [ 0.0361,  0.0690],\n",
      "        [-0.5424,  0.3774],\n",
      "        [ 0.6718, -0.4905],\n",
      "        [ 0.2405, -0.3118],\n",
      "        [-0.1895,  0.6260],\n",
      "        [-0.6619, -0.1335],\n",
      "        [ 0.2543,  0.3212],\n",
      "        [-0.2764, -0.3860],\n",
      "        [ 0.1426, -0.6437],\n",
      "        [ 0.0939, -0.4472],\n",
      "        [-0.1292,  0.6365],\n",
      "        [-0.5872, -0.3497],\n",
      "        [-0.5440,  0.4215],\n",
      "        [ 0.5840, -0.4573],\n",
      "        [ 0.4462,  0.4529],\n",
      "        [-0.6075,  0.2413],\n",
      "        [ 0.3214, -0.1262],\n",
      "        [ 0.4757, -0.0439],\n",
      "        [-0.0918,  0.3967],\n",
      "        [-0.6465, -0.6945],\n",
      "        [ 0.1314,  0.0779],\n",
      "        [ 0.0501, -0.4332],\n",
      "        [ 0.5635, -0.4372],\n",
      "        [-0.3127, -0.5968],\n",
      "        [-0.7042,  0.1711],\n",
      "        [-0.2849,  0.4328],\n",
      "        [ 0.6826,  0.2297],\n",
      "        [ 0.6182, -0.4922],\n",
      "        [ 0.3725,  0.3843],\n",
      "        [-0.2830, -0.0502],\n",
      "        [-0.4243,  0.0681],\n",
      "        [-0.6755, -0.4222],\n",
      "        [-0.6095, -0.0892],\n",
      "        [ 0.0721, -0.5685],\n",
      "        [-0.2598,  0.4246],\n",
      "        [ 0.0343,  0.1239],\n",
      "        [-0.3123, -0.2080],\n",
      "        [-0.6269,  0.6587],\n",
      "        [ 0.0804,  0.3257],\n",
      "        [ 0.4134,  0.6200],\n",
      "        [-0.4877,  0.1974],\n",
      "        [-0.6129,  0.2632],\n",
      "        [-0.3628,  0.2754],\n",
      "        [-0.5206,  0.3742],\n",
      "        [ 0.5064,  0.3887],\n",
      "        [-0.5938,  0.4422],\n",
      "        [-0.4487, -0.6614],\n",
      "        [ 0.4603,  0.1204],\n",
      "        [-0.2462, -0.6181],\n",
      "        [ 0.1818, -0.3057],\n",
      "        [-0.5607, -0.0887],\n",
      "        [-0.6022,  0.3483],\n",
      "        [-0.4659, -0.2407],\n",
      "        [-0.6995, -0.0984],\n",
      "        [-0.2183,  0.3902],\n",
      "        [-0.1600,  0.4760]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.6173,  0.1442, -0.2424, -0.5825, -0.4947, -0.2382, -0.4431,  0.3744,\n",
      "        -0.2283, -0.0259,  0.0294, -0.6110,  0.5896, -0.2703, -0.5724, -0.0112,\n",
      "        -0.6215,  0.0882, -0.4576, -0.1551, -0.2080,  0.5117,  0.5457,  0.7010,\n",
      "         0.0779,  0.4510, -0.5803, -0.4743,  0.3805,  0.1553,  0.1957,  0.4042,\n",
      "         0.4483,  0.3323, -0.2619, -0.2240,  0.1489,  0.5820, -0.0536, -0.2105,\n",
      "         0.3907,  0.4688, -0.0131, -0.2091, -0.7045,  0.0434,  0.2168, -0.5191,\n",
      "        -0.5250, -0.7069,  0.5525,  0.6629,  0.6998,  0.6313, -0.0745,  0.3145,\n",
      "         0.4185, -0.1889, -0.4074,  0.5886, -0.0741,  0.2607, -0.3307, -0.3648],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0232,  0.0041,  0.0739,  0.0812, -0.0157,  0.1229, -0.0665,  0.0084,\n",
      "         -0.0783,  0.0205,  0.1139, -0.1034,  0.0294, -0.1149, -0.0280, -0.0587,\n",
      "          0.0002, -0.0867,  0.0317, -0.1179,  0.0274, -0.0482, -0.0748,  0.1060,\n",
      "          0.0135, -0.0827, -0.0705, -0.0558, -0.0740,  0.0155,  0.0018,  0.1090,\n",
      "          0.0036, -0.0667,  0.1075,  0.0314, -0.0270,  0.0877,  0.1025, -0.1052,\n",
      "         -0.1031, -0.0822, -0.1075, -0.0375,  0.0458, -0.1140, -0.1222, -0.1076,\n",
      "         -0.0652,  0.0784,  0.1108,  0.1242,  0.0014, -0.1059,  0.1219, -0.0840,\n",
      "          0.0472,  0.0625,  0.0386,  0.0413, -0.1166, -0.0421, -0.0804,  0.0863],\n",
      "        [-0.0710, -0.0030,  0.1111,  0.0947, -0.0899, -0.1043,  0.0989,  0.0360,\n",
      "         -0.0958,  0.1081, -0.0902,  0.1082,  0.0897,  0.0891, -0.0240,  0.0109,\n",
      "         -0.1044,  0.0704,  0.0072,  0.0926, -0.0527,  0.1207,  0.0833, -0.0350,\n",
      "          0.0341, -0.0912, -0.0832, -0.0856, -0.0718,  0.0348, -0.0539, -0.0432,\n",
      "          0.0874,  0.0206, -0.0609, -0.0339, -0.0329,  0.0584,  0.0880,  0.0996,\n",
      "         -0.0850, -0.0134,  0.0293, -0.0489,  0.0822, -0.0139,  0.0133, -0.1069,\n",
      "          0.0903, -0.0828, -0.0030, -0.1020,  0.0951,  0.0072,  0.0563, -0.0982,\n",
      "          0.0106,  0.0512,  0.0883,  0.0919, -0.0223, -0.0127,  0.0415, -0.0449]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0509, -0.1151], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.5413, 0.5413], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.4896, -0.1563],\n",
      "        [-0.5041,  0.0180],\n",
      "        [ 0.1113,  0.6924],\n",
      "        [ 0.2038, -0.0041],\n",
      "        [-0.5139, -0.2842],\n",
      "        [-0.1153, -0.5092],\n",
      "        [ 0.1376,  0.5089],\n",
      "        [ 0.4046, -0.2933],\n",
      "        [-0.1729,  0.7049],\n",
      "        [-0.1132, -0.6351],\n",
      "        [-0.0062,  0.0360],\n",
      "        [-0.2627, -0.0768],\n",
      "        [-0.5766,  0.2178],\n",
      "        [-0.4907,  0.2434],\n",
      "        [ 0.5328, -0.4565],\n",
      "        [-0.2769,  0.0680],\n",
      "        [ 0.0017,  0.4146],\n",
      "        [-0.0424,  0.1143],\n",
      "        [-0.1955,  0.2332],\n",
      "        [ 0.2136, -0.5693],\n",
      "        [ 0.3754, -0.1449],\n",
      "        [ 0.6332, -0.7017],\n",
      "        [-0.0702,  0.3195],\n",
      "        [ 0.4876,  0.0092],\n",
      "        [-0.3821, -0.5466],\n",
      "        [-0.5355,  0.5693],\n",
      "        [-0.6158, -0.2798],\n",
      "        [ 0.3707,  0.3867],\n",
      "        [-0.6185,  0.1402],\n",
      "        [ 0.0855,  0.6490],\n",
      "        [-0.0251,  0.3003],\n",
      "        [-0.0611,  0.6312],\n",
      "        [-0.0722, -0.4155],\n",
      "        [ 0.1304, -0.3743],\n",
      "        [ 0.1768,  0.4915],\n",
      "        [-0.5500, -0.5144],\n",
      "        [ 0.0096, -0.0414],\n",
      "        [-0.2201,  0.5919],\n",
      "        [ 0.3170, -0.2344],\n",
      "        [ 0.1025, -0.1064],\n",
      "        [-0.4527,  0.2893],\n",
      "        [-0.4665,  0.0856],\n",
      "        [ 0.4800, -0.6494],\n",
      "        [ 0.0310,  0.6530],\n",
      "        [ 0.3193, -0.2789],\n",
      "        [ 0.1113, -0.6639],\n",
      "        [ 0.1135,  0.6148],\n",
      "        [ 0.6145, -0.0113],\n",
      "        [ 0.2564, -0.1174],\n",
      "        [-0.1985,  0.0136],\n",
      "        [-0.4119,  0.2847],\n",
      "        [ 0.3467, -0.1917],\n",
      "        [-0.1609, -0.4927],\n",
      "        [ 0.0904,  0.2812],\n",
      "        [ 0.6713, -0.6663],\n",
      "        [-0.5962,  0.0200],\n",
      "        [ 0.6535,  0.0455],\n",
      "        [ 0.1963,  0.1004],\n",
      "        [-0.0846,  0.5047],\n",
      "        [ 0.5191, -0.4077],\n",
      "        [-0.4235, -0.5498],\n",
      "        [-0.1720, -0.0159],\n",
      "        [ 0.5918, -0.6844],\n",
      "        [ 0.5019, -0.3950]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0578, -0.0330,  0.3994, -0.5915, -0.4938,  0.0162,  0.3566, -0.3402,\n",
      "         0.2192, -0.1869, -0.6594,  0.0807, -0.1507,  0.4183,  0.4658,  0.4626,\n",
      "         0.4429, -0.5471, -0.6784,  0.0790,  0.2053, -0.5439,  0.5779, -0.0749,\n",
      "        -0.4144,  0.4850,  0.6193,  0.1086, -0.2255,  0.0767, -0.5212,  0.4358,\n",
      "         0.4439, -0.1324, -0.5606,  0.5249, -0.5785, -0.3713,  0.4218, -0.3514,\n",
      "         0.1347,  0.5812, -0.0894, -0.1599,  0.6555,  0.1208,  0.2466, -0.2324,\n",
      "        -0.6100,  0.1107, -0.4180, -0.3657, -0.5800,  0.6058, -0.4746,  0.2046,\n",
      "         0.1455,  0.4884,  0.3862, -0.4951,  0.3134, -0.3592,  0.0260, -0.2561],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1070,  0.0374, -0.0834,  0.1223,  0.0504, -0.0432, -0.0732, -0.1152,\n",
      "         -0.0630, -0.0816,  0.0134,  0.0299,  0.1084,  0.0168, -0.0254, -0.0557,\n",
      "         -0.0176,  0.0661, -0.1205, -0.1144,  0.0493, -0.0386, -0.0370, -0.0805,\n",
      "         -0.0832, -0.0936, -0.0761, -0.1143, -0.0587,  0.1003, -0.0692,  0.0950,\n",
      "         -0.1052,  0.0145, -0.0104, -0.0944,  0.0030,  0.1041, -0.0565,  0.1148,\n",
      "         -0.0131, -0.0395,  0.0617,  0.0977,  0.1079,  0.0740, -0.1117, -0.0255,\n",
      "          0.0136, -0.0570,  0.0584, -0.0275, -0.1077, -0.0553, -0.0806,  0.0479,\n",
      "         -0.0910, -0.0315,  0.0129,  0.0539, -0.0961,  0.0862, -0.0802, -0.0958],\n",
      "        [ 0.0511, -0.0733,  0.0907,  0.0539, -0.0607, -0.0274, -0.0571, -0.0119,\n",
      "         -0.0003, -0.0030, -0.0817, -0.1042, -0.0799,  0.0211,  0.0128, -0.0629,\n",
      "         -0.0203, -0.0031,  0.0894, -0.1123, -0.0557, -0.0178, -0.0205, -0.0660,\n",
      "          0.0622, -0.0470,  0.0129, -0.0711,  0.1239, -0.0738, -0.0262,  0.1002,\n",
      "          0.0679, -0.1127,  0.0794, -0.0667, -0.0153,  0.0511,  0.0081,  0.0962,\n",
      "         -0.0471, -0.0873,  0.0079,  0.0430,  0.0941, -0.0635, -0.0140, -0.0722,\n",
      "         -0.0907, -0.0977, -0.0885, -0.0754,  0.0411, -0.0172,  0.0786,  0.0958,\n",
      "         -0.0969,  0.0242,  0.0684, -0.1011, -0.0361,  0.0908, -0.0924,  0.1117]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.0107, 0.0227], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(reverses)\n",
    "\n",
    "_ = [print(p) for f in reverses for p in f.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from combinators.objectives import mb0, mb1, _estimate_mc, eval_nrep\n",
    "optimizer = torch.optim.Adam([dict(params=x.parameters()) for x in [*forwards, *reverses]], lr=1e-2)\n",
    "\n",
    "lazy_i, i = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-115983a667ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlazy_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mtrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlazy_i\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nix/store/1fw9h2capcs2dpc7pgc39pn3g0zwm078-python3-3.8.6-env/lib/python3.8/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36mtnrange\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0mOn\u001b[0m \u001b[0mPython3\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mused\u001b[0m \u001b[0minstead\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mxrange\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \"\"\"\n\u001b[0;32m--> 287\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_range\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nix/store/1fw9h2capcs2dpc7pgc39pn3g0zwm078-python3-3.8.6-env/lib/python3.8/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0munit_scale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munit_scale\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munit_scale\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0munit_scale\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         self.container = self.status_printer(\n\u001b[0m\u001b[1;32m    219\u001b[0m             self.fp, total, self.desc, self.ncols)\n\u001b[1;32m    220\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nix/store/1fw9h2capcs2dpc7pgc39pn3g0zwm078-python3-3.8.6-env/lib/python3.8/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36mstatus_printer\u001b[0;34m(_, total, desc, ncols)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;31m# Prepare IPython progress bar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mIProgress\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# #187 #451 #558 #872\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             raise ImportError(\n\u001b[0m\u001b[1;32m     97\u001b[0m                 \u001b[0;34m\"IProgress not found. Please update jupyter and ipywidgets.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0;34m\" See https://ipywidgets.readthedocs.io/en/stable\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "source": [
    "num_iterations=5000\n",
    "lazy_i = i\n",
    "\n",
    "with trange(num_iterations) as bar:\n",
    "    for i in bar:\n",
    "        i += lazy_i\n",
    "        q0 = targets[0]\n",
    "        p_prv_tr, out0 = q0(sample_shape=sample_shape)\n",
    "\n",
    "        loss = torch.zeros(1, **kw_autodevice())\n",
    "        lw, lvss = torch.zeros(sample_shape, **kw_autodevice()), []\n",
    "        for k, (fwd, rev, q, p) in enumerate(zip(forwards, reverses, targets[:-1], targets[1:])):\n",
    "            q.with_observations(trace_utils.copytrace(p_prv_tr, detach=p_prv_tr.keys()))\n",
    "            q_ext = Forward(fwd, q, _step=k)\n",
    "            p_ext = Reverse(p, rev, _step=k)\n",
    "            extend = Propose(target=p_ext, proposal=q_ext, _step=k)\n",
    "#             breakpoint()\n",
    "            state, lv = extend(sample_shape=sample_shape, sample_dims=0)\n",
    "\n",
    "            p_prv_tr = state.target.trace\n",
    "            p.clear_observations()\n",
    "            q.clear_observations()\n",
    "            lw += lv\n",
    "#             loss += nvo_rkl(lw, lv, state.proposal.trace[f'g{k}'], state.target.trace[f'g{k+1}'])\n",
    "\n",
    "            loss += nvo_avo(lv)\n",
    "\n",
    "# # # # #             breakpoint()s\n",
    "#             batch_dim=None\n",
    "#             sample_dims=0\n",
    "#             rv_proposal=state.proposal.trace[f'g{k}']\n",
    "#             rv_target=state.target.trace[f'g{k+1}']\n",
    "#             # TODO: move back from the proposal and target RVs to joint logprobs?\n",
    "#             reducedims = (sample_dims,)\n",
    "\n",
    "#             lw = lw.detach()\n",
    "#             ldZ = lv.detach().logsumexp(dim=sample_dims) - math.log(lv.shape[sample_dims])\n",
    "#             f = -lv\n",
    "\n",
    "#             # rv_proposal = next(iter(proposal_trace.values())) # tr[\\gamma_{k-1}]\n",
    "#             # rv_target = next(iter(target_trace.values()))     # tr[\\gamma_{k}]\n",
    "\n",
    "#             kwargs = dict(\n",
    "#                 sample_dims=sample_dims,\n",
    "#                 reducedims=reducedims,\n",
    "#                 keepdims=False\n",
    "#             )\n",
    "\n",
    "#             baseline = _estimate_mc(f.detach(), lw, **kwargs).detach()\n",
    "\n",
    "#             kl_term = _estimate_mc(mb1(rv_proposal.log_prob.squeeze()) * (f - baseline), lw, **kwargs)\n",
    "\n",
    "#             grad_log_Z1 = _estimate_mc(rv_proposal.log_prob.squeeze(), lw, **kwargs)\n",
    "#             grad_log_Z2 = _estimate_mc(eval_nrep(rv_target).log_prob.squeeze(), lw+lv.detach(), **kwargs)\n",
    "#            #s breakpoint()\n",
    "\n",
    "#             if k==0:\n",
    "# #                 loss += kl_term + mb0(baseline * grad_log_Z1 - grad_log_Z2) + baseline + ldZ\n",
    "#                 loss += nvo_avo(lv)\n",
    "\n",
    "            lvss.append(lv)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "#         scheduler.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # REPORTING\n",
    "            # ---------------------------------------\n",
    "#             # ESS\n",
    "            lvs = torch.stack(lvss, dim=0)\n",
    "            lws = torch.cumsum(lvs, dim=1)\n",
    "            ess = effective_sample_size(lws, sample_dims=-1)\n",
    "            for step, x in zip(range(1,len(ess)+1), ess):\n",
    "                writer.add_scalar(f'ess/step-{step}', x, i)\n",
    "\n",
    "            # logZhat\n",
    "            lzh = log_Z_hat(lws, sample_dims=-1)\n",
    "            for step, x in zip(range(1,len(lzh)+1), lzh):\n",
    "                writer.add_scalar(f'log_Z_hat/step-{step}', x, i)\n",
    "\n",
    "            # loss\n",
    "            loss_ct += 1\n",
    "            loss_scalar = loss.detach().cpu().mean().item()\n",
    "            writer.add_scalar('loss', loss_scalar, i)\n",
    "            loss_sum += loss_scalar\n",
    "\n",
    "            # progress bar\n",
    "            if i % 10 == 0:\n",
    "                loss_avg = loss_sum / loss_ct\n",
    "                loss_template = 'loss={}{:.4f}'.format('' if loss_avg < 0 else ' ', loss_avg)\n",
    "                logZh_template = 'logZhat[-1]={:.4f}'.format(lzh[-1].cpu().item())\n",
    "                ess_template = 'ess[-1]={:.4f}'.format(ess[-1].cpu().item())\n",
    "                loss_ct, loss_sum  = 0, 0.0\n",
    "                bar.set_postfix_str(\"; \".join([loss_template, ess_template, logZh_template]))\n",
    "\n",
    "#             show samples\n",
    "            if i % (eval_break + 1) == 0:\n",
    "                samples = sample_along(targets[0], forwards)\n",
    "                fig = V.scatter_along(samples)\n",
    "                writer.add_figure('overview', fig, global_step=i, close=True)\n",
    "#                 for ix, xs in enumerate(samples):\n",
    "#                     writer.add_figure(f'step-{ix+1}', V.scatter(xs), global_step=i, close=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = sample_along(targets[0], forwards)\n",
    "plot_type = len(samples[0].squeeze().shape)\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from scipy.interpolate import interpn\n",
    "from matplotlib import cm\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "\n",
    "\n",
    "def scatter(xs, lws=None, c='C0', ax=None, show=False):\n",
    "    xs = xs.squeeze().detach().cpu().numpy()\n",
    "    assert len(xs.shape) == 2\n",
    "    inplace = ax is not None\n",
    "    cm_endpoints = [(i, (*colors.to_rgb(c), i)) for i in [0.0, 1.0]]\n",
    "    lin_alpha = colors.LinearSegmentedColormap.from_list('incr_alpha', cm_endpoints)\n",
    "    fig = None\n",
    "#     if ax is None:\n",
    "#         fig, ax = plt.subplots()\n",
    "\n",
    "    plt.scatter(*xs.T, c=None if lws is None else lws.softmax(dim=0), cmap=lin_alpha)\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "    return fig if fig is not None else ax\n",
    "\n",
    "def scatter_together(samples):\n",
    "    fig = plt.figure(figsize=(5*len(samples), 5))\n",
    "    gspec = gridspec.GridSpec(ncols=len(samples), nrows=1, figure=fig)\n",
    "\n",
    "    for i, xs in enumerate(samples):\n",
    "        ax = fig.add_subplot(gspec[0, i])\n",
    "        scatter(xs)\n",
    "    return fig\n",
    "\n",
    "if plot_type == 1:\n",
    "    print(\";  \".join([\"{:.4f}\".format(ss.mean().cpu().item()) for ss in samples]))\n",
    "elif plot_type == 2:\n",
    "    fig = scatter_together(samples)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_along()\n",
    "g0 = targets[0]\n",
    "tr0, o0 = g0(sample_shape=(2000,))\n",
    "plt.scatter(tr0[g0.name].value[:,0], tr0[g0.name].value[:,1], c=g.log_density_fn(tr0[g0.name].value), cmap='magma')\n",
    "\n",
    "\n",
    "g = targets[-1]\n",
    "tr, o = g.sample(sample_shape=(2000,))\n",
    "plt.scatter(tr[g.name].value[:,0], tr[g.name].value[:,1], c=tr[g.name].log_prob, cmap='viridis')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g0 = targets[0]\n",
    "tr0, o0 = g0(sample_shape=(2000,))\n",
    "plt.scatter(tr0[g0.name].value[:,0], tr0[g0.name].value[:,1], c=g.log_density_fn(tr0[g0.name].value), cmap='magma')\n",
    "\n",
    "\n",
    "g = targets[-1]\n",
    "tr, o = g.sample(sample_shape=(2000,))\n",
    "plt.scatter(tr[g.name].value[:,0], tr[g.name].value[:,1], c=tr[g.name].log_prob, cmap='viridis')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reverses)\n",
    "\n",
    "_ = [print(p) for f in reverses for p in f.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
